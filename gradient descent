import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
X = np.asarray([20,5,10,13,12])
Y = np.asarray ([18,6,13,12,13])
b1, bo = 0, 0

L=0.001
epochs = 20
n = np.size(X)
b1_history=[]
loss_history=[]
for i in range(epochs):
  Y_pred=(b1*X+b0)
  loss=(1/n)*sum((Y-Y_pred) **2)
  b1_history.append(b1)
  loss_history.append(loss)
  D_b1=(-2/n)*sum(X*(Y-Y_pred))
  D_b0=(-2/n)*sum(Y-Y_pred)
  b1-=L*D_b1
  bo-=L*D_b0
print(f"Final coefficients: b1={b1}, b0={b0}")
Y_pred=(b1*X+b0)
mse=mean_squared_error(Y,Y_pred)
print("MSE",mse)
cd=r2_score(Y,Y_pred)
print("Coefficent of determination ",cd)

#Visualization 1: Actual vs Predicted

plt.figure(figsize=(8, 6))
plt.plot(range(n), Y, 'o-', label="Actual", color="blue")
plt.plot(range(n), Y_pred, 'x--', label="Predicted", color="red")
plt.title("Actual vs Predicted Values")
plt.xlabel("Index")
plt.ylabel("Y")
plt.legend()
plt.grid()
plt.show()

# Visualization 2: Gradient Descent Progression

plt.figure(figsize=(10, 6))
plt.plot(loss_history, label='Loss')
plt.title('Gradient Descent Progression')
plt.xlabel('Iteration')
plt.ylabel('Loss (Mean Squared Error)')
plt.legend()
plt.grid()
plt.show()
